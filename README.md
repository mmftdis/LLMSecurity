# LLMSecurity

LLMSecurity: Secure Your LLM Workflows

LLMSecurity is a dedicated cyber-security tool built to safeguard operations leveraging Large Language Models (LLMs). It proactively defends against common threats, including:

- Prompt Injection Protection: Mitigates attacks where malicious prompts manipulate the LLM's behavior, originating from both human users and automated systems.
- Agent Injection Protection: Prevents unauthorized tools and extensions from being injected into the LLM workflow.
- RAG Injection Protection: Protects against the risky embedding of malicious documents within Retrieval-Augmented Generation (RAG) systems.

*Coming Soon...*

